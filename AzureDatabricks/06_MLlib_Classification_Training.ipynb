{"cells":[{"cell_type":"markdown","source":["# Developing machine learning classification model with Spark MLlib\n\nIn this lab you will learn how to develop a machine learning classification model using Spark MLlib in Azure Databricks environment. During the course of the lab you will walk through cardinal phases of a machine learning workflow from data gathering and cleaning through feature engineering and modeling to model inferencing.\n\n## Lab scenario\nYou will develop a machine learning classification model to predict customer churn. The dataset used during the lab contains historical information about customers of a fictional telecomunication company. You will use Azure Databricks unified analytics platform and Spark MLlib library to implement the ML workflow resulting in a customer churn prediction model."],"metadata":{}},{"cell_type":"markdown","source":["## MLlib Overview\n\n### What is MLlib?\n\nMLlib is a package, built on and included in Spark, that provides interfaces for\n- gathering and cleaning data,\n- feature engineering and feature selection,\n- training and tuning large scale supervised and unsupervised machine learning models, \n- and using those models in production.\n\n### MLlib Concepts\n\n![MLlib](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/MLlib.png)"],"metadata":{}},{"cell_type":"markdown","source":["## Gather, Analyze, and Preprocess data\n\n### Load and review data\n\nWe begin by loading and doing a rudimentary analysis of customer churn historical data, which is stored in CSV format in Azure Blob."],"metadata":{}},{"cell_type":"code","source":["# Reset the widgets\ndbutils.widgets.removeAll()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Set up notebook parameters\ndbutils.widgets.text(\"STORAGE_ACCOUNT\", \"azureailabs\")\ndbutils.widgets.text(\"CONTAINER\", \"churn\")\ndbutils.widgets.text(\"ACCOUNT_KEY\", \"\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Load data from Azure Blob\nSTORAGE_ACCOUNT = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\nCONTAINER = dbutils.widgets.get(\"CONTAINER\").strip()\nACCOUNT_KEY = dbutils.widgets.get(\"ACCOUNT_KEY\").strip()\n\nif ACCOUNT_KEY != \"\":\n  # Set up account access key\n  conf_key = \"fs.azure.account.key.{storage_acct}.blob.core.windows.net\".format(storage_acct=STORAGE_ACCOUNT)\n  spark.conf.set(conf_key, ACCOUNT_KEY)\n\nsource_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\n  \n# Read the data from the default datasets repository in Databricks\ndf = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(source_str)\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["The `Churn` column indicates whether the customer changed providers. This is our `target` variable or `label`. The goal of our model is to predict this column on new examples.\n\nThe subset of other columns will be used as predictors or features.\n\nSome of the columns - e.g. `customerid` and `callingnum` - are not good candidates for features. They don't capture much information about the customer profile and may *leak the target* in the model. We will remove them from the training dataset.\n\nThere also some suspicious records. The first two records indicate that a 12 year old makes over $160,000 a year. Although it is possible it is highly improbable.\n\nLet's drill down a little bit."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import when\n\ndisplay(\n  df.withColumn(\"agegroup\", \n                when(df.age<=13, '0-13')\n                .when((df.age>13) & (df.age<18), '14-16')\n                .otherwise('>17'))\n  .withColumn(\"incomegroup\",\n                  when(df.annualincome==0, 'O')\n                  .when((df.annualincome>0) & (df.annualincome<10000) , '<10K')\n                  .otherwise('>10K'))\n  .groupBy('agegroup', 'incomegroup').count()\n)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["There 583 records of young kids making mor than $10,000. For the sake of this lab we will assume that these are errorneous records and remove them.\n\nWe will now create a DataFrame with an explicitly defined schema. We will also remove irrelevant columns and suspicious rows."],"metadata":{}},{"cell_type":"markdown","source":["#### Cleanse data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nschema = StructType([\n  StructField(\"age\", DoubleType()),\n  StructField(\"annualincome\", DoubleType()),\n  StructField(\"calldroprate\", DoubleType()),\n  StructField(\"callfailurerate\", DoubleType()),\n  StructField(\"callingnum\", StringType()),\n  StructField(\"customerid\", StringType()),\n  StructField(\"customersuspended\",  StringType()),\n  StructField(\"education\",  StringType()),\n  StructField(\"gender\", StringType()),\n  StructField(\"homeowner\", StringType()),\n  StructField(\"maritalstatus\", StringType()),\n  StructField(\"monthlybilledamount\", DoubleType()),\n  StructField(\"noadditionallines\", StringType()),\n  StructField(\"numberofcomplaints\", DoubleType()),\n  StructField(\"numberofmonthunpaid\", DoubleType()),\n  StructField(\"numdayscontractequipmentplanexpiring\", DoubleType()),\n  StructField(\"occupation\", StringType()),\n  StructField(\"penaltytoswitch\", DoubleType()),\n  StructField(\"state\", StringType()),\n  StructField(\"totalminsusedinlastmonth\", DoubleType()),\n  StructField(\"unpaidbalance\", DoubleType()),\n  StructField(\"usesinternetservice\", StringType()),\n  StructField(\"usesvoiceservice\", StringType()),\n  StructField(\"percentagecalloutsidenetwork\", DoubleType()),\n  StructField(\"totalcallduration\", DoubleType()),\n  StructField(\"avgcallduration\", DoubleType()),\n  StructField(\"churn\", DoubleType()),\n  StructField(\"year\", DoubleType()),\n  StructField(\"month\", DoubleType())\n])\n\ndf = (spark.read\n     .option(\"header\", True)\n     .schema(schema)\n     .csv(source_str))\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["clean_df = (df.drop(\"customerid\", \"callingnum\", \"year\", \"month\")\n    .dropDuplicates()\n    .filter(~ ((df.age<14) & (df.annualincome>10000))))\n  \ndisplay(clean_df.groupBy(\"churn\").count())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["As shown by the last query, our dataset is unbalanced with respect to the class label. We will have to take it under consideration when training the model."],"metadata":{}},{"cell_type":"markdown","source":["#### Split data into training and test sets\n\nAt this point we will split our dataset into separate training and test sets."],"metadata":{}},{"cell_type":"code","source":["# Split the dataset randomly into 85% for training and 15% for testing.\n\ntrain, test = clean_df.randomSplit([0.85, 0.15], 0)\nprint(\"We have {} training examples and {} test examples.\".format(train.count(), test.count()))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### Visualize our data\n\nNow that we have preprocessed our features and prepared a training dataset, we can use visualizations to get more insights about the data.\n\nCalling `display()` on a DataFrame in Databricks and clicking the plot icon below the table will let you draw and pivot various plots.  See the [Visualizations section of the Databricks Guide](https://docs.databricks.com/user-guide/visualizations/index.html) for more ideas."],"metadata":{}},{"cell_type":"code","source":["display(train)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["You can also use other visualization libraries."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Get a sample of data\nsample = train.sample(False, 0.05, 42).toPandas()\n\nax = sample.plot.scatter(x='percentagecalloutsidenetwork', y='avgcallduration')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Save training and testing data\n\nAt this point, we are going to save the datasets using `Parquet` format"],"metadata":{}},{"cell_type":"code","source":["test.write.mode(\"overwrite\").parquet(\"/datasets/churn_test_data\")\ntrain.write.mode(\"overwrite\").parquet(\"/datasets/churn_train_data\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["d ### Train a Machine Learning Pipeline\n\nNow that we have understood our data and prepared it as a DataFrame with pre-processed data, we are ready to train an ML classifier. In this lab we will focus on a single algorithm - Gradient-boosted tree classifier - however in most cases you should go through a more thorough model selection process to find an algorithm that best fits you scenario and training data. We will also demonstrate how to automate hyperparameter tuning using Spark ML validators.\n\nTo achieve it, we will put together a simple Spark ML Pipeline.\n\nMost Spark ML algorithms, including GBT, expect the training data to be provided as a *numeric* column to represent the label and a column of type *Vector* to represent the features. \n\nThe features in our datasets are a mix of *numeric* and *string* values. *String* columns represent categorical features. Most *numeric* columns are continous features. Before we can configure hyper parameter tuning and Random Forest stages of our pipeline we will need to add a few data transformation steps.\n\nOur complete pipeline has the following stages:\n\n* `StringIndexer`: Convert string columns to categorical features\n* `VectorAssembler`: Assemble the feature columns into a feature vector.\n* `VectorIndexer`: Identify columns which should be treated as categorical. This is done heuristically, identifying any column with a small number of distinct values as being categorical.  For us, this will include columns like `occupation` or `homeowner` .\n* `Classifier`: This stage will train the classification algorithm.\n* `CrossValidator`: The machine learning algorithms have several [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_optimization), and tuning them to our data can improve performance of the model.  We will do this tuning using Spark's [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_&#40;statistics&#41;) framework, which automatically tests a grid of hyperparameters and chooses the best.\n\n![Image of Pipeline](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/pipeline.png)"],"metadata":{}},{"cell_type":"markdown","source":["First, we define the feature processing stages of the Pipeline:\n* Convert string columns to categorical features. \n* Assemble feature columns into a feature vector. \n* Identify categorical features, and index them.\n![Image of feature processing](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/features.png)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\nfrom pyspark.sql.types import *\nfrom pyspark.ml import Pipeline\n\n# Create a list of string indexers - one for each string column\nstringCols = [field.name for field in train.schema if field.dataType == StringType()]\nstringIndexers = [StringIndexer().setInputCol(name).setOutputCol(name+\"_idx\") for name in stringCols]\n\n# Get a list of all numeric columns\nnumericCols = [field.name for field in train.schema if field.dataType != StringType()]\n\n# Remove a label column\nnumericCols.remove('churn')\n\n# Create a list of all feature columns\nfeatureCols = numericCols + [name + \"_idx\" for name in stringCols]\n\n# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\nvectorAssembler = VectorAssembler(inputCols=featureCols, outputCol=\"rawFeatures\")\n\n# This identifies categorical features and indexes them.\nvectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n\n# Create a pipeline\nstages = stringIndexers + [vectorAssembler, vectorIndexer]\npipeline = Pipeline(stages=stages)\n\n# Check the Pipeline operation\ndisplay(pipeline.fit(train).transform(train))\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Second, we define the model training stage of the Pipeline. `GBTClassifier` takes feature vectors and labels as input and learns to predict labels of new examples.\n![RF image](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/train.png)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\n# Takes the \"features\" column and learns to predict \"churn\"\nclassifier = GBTClassifier(labelCol=\"churn\", featuresCol=\"features\", maxBins=50)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Third, we wrap the model training stage within a `CrossValidator` stage.  `CrossValidator` knows how to call the classifier algorithm with different hyperparameter settings.  It will train multiple models and choose the best one, based on minimizing some metric.  In this lab, our metric is *AUC*.\n\n![Crossvalidate](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/tune.png)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Define a grid of hyperparameters to test:\n#  - maxDepth: max depth of each decision tree in the GBT ensemble\n#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\n# In this example notebook, we keep these values small.  In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher) and more trees in the ensemble (>100).\nparamGrid = ParamGridBuilder()\\\n  .addGrid(classifier.maxDepth, [5, 7])\\\n  .addGrid(classifier.maxIter, [10, 50])\\\n  .build()\n# Define a binary evaluator\nevaluator = BinaryClassificationEvaluator(labelCol=classifier.getLabelCol(), rawPredictionCol=classifier.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=classifier, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=3)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Finally, we can tie our feature processing and model training stages together into a single `Pipeline`.\n\n![Image of Pipeline](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/pipeline.png)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\nstages = pipeline.getStages()\nstages = stages + [cv]\npipeline.setStages(stages)\nprint(pipeline.getStages())"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["#### Train the Pipeline!\n\nNow that we have set up our workflow, we can train the Pipeline in a single call.  Calling `fit()` will run feature processing, model tuning, and training in a single call.  We get back a fitted Pipeline with the best model found.\n\n***Note***: This next cell can take up to **10 minutes**.  This is because it is training *a lot* of trees:\n* For each random sample of data in Cross Validation,\n  * For each setting of the hyperparameters,\n    * `CrossValidator` is training a separate GBT ensemble which contains many Decision Trees.\n    \nSince our training set is unbalanced we will apply a technique called *under sampling*. We will use all instances of a minority class but select a random sample from the majority class."],"metadata":{}},{"cell_type":"code","source":["# Load training data\ntrain = spark.read.parquet(\"/datasets/churn_train_data\")\n\n# Undersample majority class\nstratified_train = train.sampleBy('Churn', fractions={0: 0.2, 1: 1.0}).cache()\n\ndisplay(stratified_train.groupby('Churn').count())"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Start training."],"metadata":{}},{"cell_type":"code","source":["pipelineModel = pipeline.fit(stratified_train)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["## Make predictions, and evaluate results\n\nOur final step will be to use our fitted model to make predictions on new data.  We will use our held-out test set, but you could also use this model to make predictions on completely new data.  \n\nWe will also evaluate our predictions.  Computing evaluation metrics is important for understanding the quality of predictions, as well as for comparing models and tuning parameters."],"metadata":{}},{"cell_type":"markdown","source":["Calling `transform()` on a new dataset passes that data through feature processing and uses the fitted model to make predictions.  We get back a DataFrame with a new column `predictions` (as well as intermediate results such as our `rawFeatures` column from feature processing)."],"metadata":{}},{"cell_type":"code","source":["test = spark.read.parquet(\"/datasets/churn_train_data\")\n\npredictions = pipelineModel.transform(test).cache()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["It is easier to view the results when we limit the columns displayed to:\n* `churn`: the true churn indicator\n* `prediction`: our predicted churn\n* feature columns: our original (human-readable) feature columns"],"metadata":{}},{"cell_type":"code","source":["display(predictions.select(\"churn\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Calculate classification performance metrics.\n\n![Confusion matrix](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/confusion.png)\n\nThe metrics we tried to optimize was *AUC* of ROC.\n\n![ROC](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/roc.png)"],"metadata":{}},{"cell_type":"code","source":["# Calculate AOC\nprint(\"{} on our test set: {}\".format(evaluator.getMetricName(), evaluator.evaluate(predictions, {})))\n"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["## Persist the model\n\nSpark MLlib supports model persistence. Key features of ML persistence include:\n- Support of all language APIs in Spark: Scala, Java, Python & R\n- Support for single models and full Pipelines, both unfitted (a \"recipe\") and fitted (a result)\n- Distributed storage using and exchangealbe format\n\nIn Azure Databricks, by default, the model is saved to and loaded from DBFS."],"metadata":{}},{"cell_type":"code","source":["model_path = '/models/churn_classifier'\npipelineModel.write().overwrite().save(model_path)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%fs ls 'dbfs:/models/churn_classifier'"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["You will use the saved model during the deployment lab."],"metadata":{}}],"metadata":{"name":"06 - MLlib Classification Training","notebookId":126212991376761},"nbformat":4,"nbformat_minor":0}
